# -*- coding: utf-8 -*-
"""Handwriting_Recognition_Image_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lsL4BTCPmLqxqq9OpoFdSUxBp0Xbo4XZ

# DESCRIPTION

Develop ML models for image classification then convert the models into embeddable TF-Lite file format on Android and iOS.

This dataset consists of more than four hundred thousand handwritten names collected through charity projects.

total_all_dataset: 413704

Dataset: [Handwriting Recognition](https://www.kaggle.com/datasets/landlord/handwriting-recognition)

# DATA PREPARATION

## Libraries
"""

import os
import pandas as pd
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pathlib

from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D
from tensorflow.keras.applications.vgg16 import preprocess_input

from google.colab import drive
drive.mount('/content/gdrive')

!pip install kaggle

os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/MyDrive/Colab Notebooks/kaggle_API_credentials"

!kaggle datasets download -d landlord/handwriting-recognition

!unzip -qq handwriting-recognition.zip -d handwriting

!ls handwriting

base_dir = '/content/handwriting'
TRAINING_DIR = base_dir
VALIDATION_DIR = base_dir

dir_trainv2 = os.path.join(base_dir, 'train_v2/train')
dir_testv2 = os.path.join(base_dir, 'test_v2/test')
dir_validationv2 = os.path.join(base_dir, 'validation_v2/validation')

def count_contents_directory(base_dir):
    total_file = 0
    total_folder = 0

    # Mengecheck seluruh isi direktori
    for root, dirs, files in os.walk(base_dir):
        total_folder += len(dirs)
        total_file += len(files)

    return (total_file, total_folder)

total_all_files, total_all_folders = count_contents_directory(base_dir)
print("total_all_files:", total_all_files)
print("total_all_folders:", total_all_folders)

print('Total dir_trainv2:', len(os.listdir(dir_trainv2)))
print('Total dir_testv2:', len(os.listdir(dir_testv2)))
print('Total dir_validationv2:', len(os.listdir(dir_validationv2)))

"""# DATA PREPROCESSING"""

img = image.load_img(dir_testv2 + '/TEST_0005.jpg')
imgplt = plt.imshow(img)

img = image.load_img(dir_testv2 + '/TEST_0020.jpg')
imgplt = plt.imshow(img)

"""## Image Augmentation"""

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    shear_range=0.2,
    zoom_range=0.2,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2,
    preprocessing_function=preprocess_input  # Apply VGG16 preprocessing
)

train_generator = train_datagen.flow_from_directory(
    TRAINING_DIR,
    target_size=(150, 150),
    batch_size=128,
    class_mode='categorical',
    subset='training'
)

test_datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

test_generator = test_datagen.flow_from_directory(
    VALIDATION_DIR,
    target_size=(150, 150),
    class_mode='categorical',
    subset='validation'
)

"""# MODELING SEQUENTIAL"""

model = tf.keras.models.Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    Conv2D(64, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    Conv2D(128, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    Conv2D(256, (3, 3), activation='relu'),
    MaxPooling2D(2, 2),
    Dropout(0.25),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

model.summary()

"""## Compiling the model"""

model.compile(
    optimizer=tf.optimizers.Adam(),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

"""## Callback"""

class myCallback(tf.keras.callbacks.Callback):
  def __init__(self, target_accuracy=0.96, consecutive_epochs=3):
        self.target_accuracy = target_accuracy
        self.consecutive_epochs = consecutive_epochs
        self.consecutive_count = 0

  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy') > 0.92 and logs.get('val_accuracy') > 0.92):
      self.model.stop_training = True
      print('\n\nCallback called --- Done training!')
      print(" Accuracy above 92% ".center(33, '-'), '\n\n')

    if logs.get('val_accuracy') >= self.target_accuracy:
      self.consecutive_count += 1
      if self.consecutive_count >= self.consecutive_epochs:
        if logs.get('accuracy') >= 0.82 and logs.get('val_accuracy') >= self.target_accuracy:
         self.model.stop_training = True
         print('\n\nCallback called --- Done training!')
         print(" val_accuracy above 96% for 3 consecutive epochs ".center(60, '-'), '\n\n')
    else:
      self.consecutive_count = self.consecutive_count

"""### Implementing a learning rate scheduler"""

def lr_scheduler(epoch, lr):
  initial_lr=0.001
  drop=0.5
  epochs_drop=10
  lr = initial_lr * np.power(drop, np.floor((1 + epoch) / epochs_drop))
  return lr

callbacks = [
    tf.keras.callbacks.LearningRateScheduler(lr_scheduler),
    myCallback()
]

"""## Training the model"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# h = model.fit(
#     train_generator,
#     steps_per_epoch=1,
#     epochs=200,
#     validation_data=test_generator,
#     validation_steps=1,
#     verbose=2,
#     callbacks=[callbacks]
# )

"""# DATA VISUALIZATION

## Accuracy Graph
"""

sns.set_style("whitegrid")
plt.figure(figsize=(10, 6))
plt.plot(h.history['accuracy'], label='Training Accuracy')
plt.plot(h.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

"""## Loss Graph"""

sns.set_style("whitegrid")
plt.figure(figsize=(10, 6))
plt.plot(h.history['loss'], label='Training Loss')
plt.plot(h.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""# FORMAT TF-Lite"""

export_dir = 'saved_model/'
tf.saved_model.save(model, export_dir)

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)
tflite_model = converter.convert()

tflite_model_file = pathlib.Path('model.tflite')
tflite_model_file.write_bytes(tflite_model)